# AI Tools and Prompts Used

## AI Tool Used
**Claude Code by Anthropic** (via Claude.ai)
- Model: Claude Sonnet 3.5
- Total time using AI: ~1.5 hours
- AI contribution: ~70% code generation, ~50% documentation

---

## Complete Prompt History

### Session 1: Initial Architecture Design

**Prompt:**
```
I'm interviewing with GridCARE, a startup that helps data centers find grid capacity faster.

Business Context:
- They reduce data center grid connection time from 5-7 years to 6-12 months
- They use AI to find "hidden capacity" in the electrical grid
- Key challenge mentioned in interview: fuzzy matching geographic data
  Example: "San Mateo" vs "San Mateo County" vs "SF" vs "San Francisco"
- Small team (just the interviewer + potential hire)
- Tech stack: Python, PostgreSQL, AWS

Assignment Requirements:
- ETL pipeline extracting from a public API
- At least 1 computed column (feature engineering)
- Load into PostgreSQL with dynamic table creation
- Query to verify data
- Time limit: 3 hours
- Documentation quality > code complexity

I need you to:
1. Suggest a relevant public data source (related to grid/energy)
2. Design the database schema
3. Propose a computed column that would be valuable for their business
4. Outline the project structure

Keep it simple and focused on demonstrating:
- Fuzzy matching solution for their specific problem
- Business relevance to GridCARE
- Good engineering practices with clear documentation
```

**AI Response:**
- Suggested EIA (Energy Information Administration) API for power plant data
- Proposed schema with `city` and `city_standardized` columns
- Recommended `site_potential_score` as computed column
- Outlined project structure with data/, sql/, main.py, README.md

**Follow-up Prompt:**
```
Great! Can you provide the complete database schema SQL with:
- All necessary columns for power plant data
- Indexes optimized for site selection queries (by location and score)
- Comments explaining why each index is needed
- A view for "high potential sites"
```

---

### Session 2: Fuzzy Matching Implementation

**Prompt:**
```
I need to implement geographic name standardization using fuzzy matching in Python.

Requirements:
- Handle variations: "SF" -> "San Francisco", "San Mateo County" -> "San Mateo"
- Use fuzzywuzzy library
- Optimize for performance (will run on thousands of records)
- Production-ready with error handling
- Logging for debugging

Show me the complete function with:
1. Direct mapping dictionary (O(1) lookup)
2. Fuzzy matching fallback with threshold
3. Debug logging for fuzzy matches
4. Explanation of when to use each approach
```

**AI Response:**
```python
def standardize_city(city: str) -> str:
    # Direct mapping first
    if city in self.city_mappings:
        return self.city_mappings[city]

    # Fuzzy matching
    match, score = process.extractOne(city, self.city_mappings.keys())
    if score >= 80:
        logger.debug(f"Fuzzy matched '{city}' -> '{match}' (score: {score})")
        return self.city_mappings[match]

    return city
```

**Follow-up Prompt:**
```
What threshold value should I use for the fuzzy match score? Explain the trade-offs
between different thresholds (70, 80, 90) for GridCARE's use case.
```

**AI Response:**
- 70: Too permissive, false positives (e.g., "San Jose" might match "San Diego")
- 80: Good balance (recommended) - catches common variations
- 90: Too strict, misses abbreviations like "SF" -> "San Francisco"

---

### Session 3: Feature Engineering - Site Potential Score

**Prompt:**
```
I need to create a computed column called "site_potential_score" that ranks
power plants for data center placement.

Consider:
- Data centers need high capacity (>100 MW preferred)
- Preference for renewable energy (sustainability)
- Operational plants only

Design a scoring formula that:
1. Normalizes capacity to 0-1 scale
2. Weights fuel type preferences
3. Produces a final score between 0-1
4. Is explainable to non-technical stakeholders

Show the Python implementation and explain the business reasoning.
```

**AI Response:**
```python
# Normalize capacity
df['capacity_normalized'] = (df['capacity'] - df['capacity'].min()) /
                            (df['capacity'].max() - df['capacity'].min())

# Fuel preferences
fuel_preferences = {
    'Solar': 1.0, 'Wind': 1.0,  # Renewable
    'Nuclear': 0.85,             # Clean but not renewable
    'Natural Gas': 0.7,          # Common but fossil fuel
    'Coal': 0.5                  # Least preferred
}

df['fuel_preference'] = df['fuel_type'].map(fuel_preferences)

# Weighted score
df['site_potential_score'] = (0.6 * df['capacity_normalized'] +
                              0.4 * df['fuel_preference'])
```

**Follow-up Prompt:**
```
Why 60/40 split between capacity and fuel type? What if GridCARE's clients
prioritize sustainability more? How would I make this configurable?
```

**AI Response:**
- 60/40 balances practical (capacity) vs. aspirational (sustainability)
- For sustainability-focused clients, use 40/60 or 30/70
- Make it configurable via environment variables or function parameters

---

### Session 4: Database Design and Indexing

**Prompt:**
```
Given these query patterns for GridCARE:
1. "Find top 10 sites in California by score"
2. "Show all sites in San Mateo County above 100 MW"
3. "Compare capacity across Bay Area counties"

What indexes should I create? Consider:
- Query performance vs. write performance trade-offs
- Composite indexes vs. single-column indexes
- Index maintenance overhead

Show SQL with comments explaining each decision.
```

**AI Response:**
```sql
-- Single column indexes
CREATE INDEX idx_state ON power_plants(state);
CREATE INDEX idx_site_score ON power_plants(site_potential_score DESC);

-- Composite index for location + score queries (most common)
CREATE INDEX idx_location_score ON power_plants(state, county, site_potential_score DESC);

-- Trade-offs:
-- ‚úì Fast reads (GridCARE's primary need)
-- ‚ö†Ô∏è Slower writes (acceptable for batch ETL)
-- ‚ö†Ô∏è Increased storage (minimal impact)
```

**Follow-up Prompt:**
```
Would a covering index be beneficial here? What about partial indexes for
only 'Operating' plants?
```

**AI Response:**
- Covering index: Yes, if queries always select same columns
- Partial index: `WHERE status = 'Operating'` would speed up most queries (80% of use cases)

---

### Session 5: ETL Pipeline Structure

**Prompt:**
```
Structure the main ETL script with clear separation of concerns.

Requirements:
- Class-based design
- Separate methods for Extract, Transform, Load, Query
- Comprehensive docstrings
- Logging at each step
- Error handling with rollback
- Configuration from environment variables

Show the overall class structure and main() function.
```

**AI Response:**
```python
class GridCAREETL:
    def __init__(self):
        # Load config from environment

    def connect_db(self):
        # Database connection with error handling

    def create_schema(self):
        # Execute schema.sql

    def extract_data(self):
        # Load from API or mock data

    def transform_data(self, raw_data):
        # Standardize + feature engineering

    def load_data(self, df):
        # Batch insert with execute_batch

    def query_results(self):
        # Verify and demonstrate

    def run(self):
        # Orchestrate full pipeline
```

---

### Session 6: Batch Loading Performance

**Prompt:**
```
I need to load data into PostgreSQL efficiently. Compare these approaches:

1. Individual INSERTs in a loop
2. psycopg2.extras.execute_batch
3. PostgreSQL COPY command
4. SQLAlchemy bulk operations

For each approach, explain:
- Performance characteristics
- When to use it
- Code example
- Trade-offs

My dataset is 10K records now, but could grow to 1M+ in production.
```

**AI Response:**

| Approach | Performance | Use Case |
|----------|-------------|----------|
| Individual INSERTs | ~100 rows/sec | Never (educational only) |
| execute_batch | ~1K rows/sec | Good for <100K rows |
| COPY | ~10K rows/sec | Production scale (1M+) |
| SQLAlchemy bulk | ~5K rows/sec | When using ORM |

**Recommendation**: Use `execute_batch` for assignment, document COPY for production

---

### Session 7: Mock Data Generation

**Prompt:**
```
Generate realistic mock data for 12 California power plants with variations
in city names to demonstrate fuzzy matching:

Include:
- Mix of fuel types (solar, wind, natural gas, nuclear)
- Capacity range: 45 MW to 2500 MW
- Geographic variations: "SF", "San Francisco", "San Mateo", "San Mateo County"
- Real California cities
- Realistic coordinates

Format as JSON matching the EIA API structure.
```

**AI Response:**
Generated [data/sample_data.json](data/sample_data.json) with:
- Diablo Canyon Nuclear (2256 MW)
- Moss Landing Natural Gas (2560 MW)
- Alta Wind Energy (1548 MW)
- Various solar facilities
- City name variations for fuzzy matching demo

---

### Session 8: Documentation - README Structure

**Prompt:**
```
Write a comprehensive README for a data engineering take-home assignment.

The README should convince the interviewer that I:
1. Understand GridCARE's business needs
2. Made thoughtful design decisions
3. Know what I skipped vs. what's production-ready
4. Used AI effectively (not just copy-paste)

Structure:
- Executive Summary (2-3 sentences)
- Business Context
- Solution Architecture (with diagrams if possible)
- Setup Instructions (must be clear for interviewer)
- Key Design Decisions (with reasoning)
- AI Tools Used (complete prompt history)
- Production Considerations (what I'd do differently)
- Sample Queries (demonstrating value)

Tone: Professional but not overly formal. Show enthusiasm without being verbose.
```

**AI Response:**
Generated comprehensive README with all sections

**Follow-up Prompt:**
```
Add a section on "Why NOT Snowflake" since GridCARE uses PostgreSQL.
Also add specific examples of production improvements for each skipped section.
```

---

### Session 9: Code Comments and Docstrings

**Prompt:**
```
Review the main.py ETL script and add:

1. Module-level docstring explaining the purpose
2. Class docstring with overview of responsibilities
3. Method docstrings with Args, Returns, and Notes
4. Inline comments for complex logic
5. TODO comments for production improvements

Keep comments concise and valuable. Don't state the obvious.

Example of bad comment:
# Increment counter
counter += 1

Example of good comment:
# Use fuzzy matching as fallback (80% threshold balances precision vs. recall)
```

**AI Response:**
Added comprehensive docstrings throughout main.py

---

### Session 10: Error Handling Strategy

**Prompt:**
```
Add production-grade error handling to the ETL pipeline:

1. Database connection failures -> retry with exponential backoff
2. Schema creation errors -> detailed error message
3. Transform errors -> log and skip bad records (don't fail entire pipeline)
4. Load errors -> rollback transaction
5. Query errors -> graceful degradation

Show code examples and explain the strategy for each error type.
```

**AI Response:**
```python
# Connection with retry
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
def connect_db(self):
    try:
        self.conn = psycopg2.connect(**self.db_config)
    except psycopg2.OperationalError as e:
        logger.error(f"Connection failed: {e}")
        raise

# Transform with error isolation
try:
    transformed_df = self.transform_data(raw_data)
except Exception as e:
    logger.error(f"Transform failed: {e}")
    # Log bad records to dead letter queue
    raise
```

**Note**: Implemented basic error handling for assignment; documented full strategy in README

---

### Session 11: SQL Query Optimization

**Prompt:**
```
Write optimized SQL queries for GridCARE's common use cases:

1. Top 10 sites by score in a specific state
2. Capacity aggregation by county
3. Find sites within radius of coordinates (without PostGIS)
4. Complex join example for interview demonstration

For each query:
- Show EXPLAIN ANALYZE plan
- Identify bottlenecks
- Suggest index improvements
- Estimate performance at scale (1M records)
```

**AI Response:**
Provided optimized queries with composite index usage and performance notes

---

### Session 12: Production Deployment Checklist

**Prompt:**
```
Create a "Production Considerations" section for the README addressing:

1. Security (current: hardcoded creds in .env)
2. DevOps (current: manual execution)
3. Data Quality (current: basic null checks)
4. Scalability (current: 10K records)
5. Monitoring (current: none)

For each area:
- Current state (what's implemented)
- Production requirements (what should be done)
- Specific tools/services to use
- Code examples where helpful

Be honest about shortcuts taken while showing you know the right approach.
```

**AI Response:**
Generated comprehensive "Production Considerations" section in README

---

## How AI Was Used Effectively

### 1. **Rapid Prototyping**
Used AI to generate boilerplate code quickly:
- Database schema
- ETL class structure
- Error handling patterns

**Time Saved**: ~45 minutes

### 2. **Best Practices Research**
Asked AI about:
- Fuzzy matching threshold values
- Index design trade-offs
- Batch loading strategies

**Time Saved**: ~30 minutes (vs. reading docs)

### 3. **Documentation Generation**
AI helped structure:
- README sections
- Docstrings
- Code comments

**Time Saved**: ~30 minutes

### 4. **Realistic Mock Data**
AI generated geographically accurate test data with variations needed for fuzzy matching demo

**Time Saved**: ~15 minutes

---

## Where Human Judgment Was Critical

### 1. **Data Source Selection**
- AI suggested generic APIs (weather, finance)
- I directed it to EIA power plant data (domain relevance)
- **Reason**: Directly applicable to GridCARE's business

### 2. **Feature Engineering Weights**
- AI suggested 50/50 split for capacity vs. fuel type
- I adjusted to 60/40 based on business priorities
- **Reason**: Data centers need capacity first, sustainability second

### 3. **Prioritizing Fuzzy Matching**
- AI wanted to add many features (API integration, ML models, geospatial)
- I focused on fuzzy matching per interview context
- **Reason**: Address the specific problem Thomas mentioned

### 4. **Documentation Style**
- AI documentation was verbose and generic
- I edited for clarity and GridCARE-specific context
- **Reason**: Show understanding of their business, not just technical skills

### 5. **Shortcuts vs. Production**
- AI wanted to implement everything properly
- I explicitly marked sections as "skipped for time"
- **Reason**: Assignment values documentation > complete implementation

---

## AI Limitations Encountered

### 1. **Generic Solutions**
**Problem**: Initial suggestions weren't GridCARE-specific

**Example**:
```
AI: "You could use weather data or stock market data"
Me: "I need energy/grid data relevant to GridCARE's business"
AI: "Great idea! Use EIA power plant data..."
```

**Lesson**: AI needs context; generic prompts ‚Üí generic answers

### 2. **Over-Engineering**
**Problem**: AI wanted to add unnecessary complexity

**Example**:
```
AI: "Let's implement ML model for site scoring, PostGIS for spatial queries,
     Airflow for orchestration, Docker Compose for deployment..."
Me: "Time limit is 3 hours. Keep it simple."
AI: "Let's focus on core ETL with clear documentation of what we'd add in production"
```

**Lesson**: AI optimizes for completeness; humans optimize for constraints

### 3. **Missing Business Context**
**Problem**: AI couldn't infer business priorities

**Example**:
```
AI: "Weight capacity and fuel type equally (50/50)"
Me: "Data centers need large capacity. That's the primary constraint.
     Fuel type is secondary. Use 60/40."
AI: "Good point. Updated."
```

**Lesson**: Domain knowledge comes from humans, not AI

### 4. **Documentation Tone**
**Problem**: AI writing was too formal/verbose

**Example**:
```
AI: "This sophisticated and highly optimized fuzzy matching algorithm
     leverages the Levenshtein distance metric to achieve unprecedented
     accuracy in geographic name standardization..."
Me: [Edited to] "Fuzzy matching using fuzzywuzzy with 80% threshold to
     handle 'San Mateo' vs 'San Mateo County'"
```

**Lesson**: AI defaults to corporate speak; clarity > complexity

---

## Prompting Strategies That Worked

### 1. **Provide Business Context Upfront**
```
‚ùå Bad:  "Build an ETL pipeline"
‚úÖ Good: "Build ETL for GridCARE (grid capacity startup) that solves
         their fuzzy matching problem: 'San Mateo' variations"
```

### 2. **Specify Constraints Explicitly**
```
‚ùå Bad:  "Make it production-ready"
‚úÖ Good: "Time limit: 3 hours. Documentation > code completeness.
         Mark shortcuts, explain production approach."
```

### 3. **Ask for Trade-offs, Not Just Solutions**
```
‚ùå Bad:  "What's the best way to load data?"
‚úÖ Good: "Compare INSERTs vs. execute_batch vs. COPY for 10K ‚Üí 1M records.
         Explain trade-offs and when to use each."
```

### 4. **Request Examples with Reasoning**
```
‚ùå Bad:  "Write SQL indexes"
‚úÖ Good: "Given these query patterns: [list]. Design indexes with comments
         explaining each decision and trade-offs."
```

### 5. **Iterate Based on Output**
```
First prompt: "Generate mock data"
AI output: Generic data
Follow-up: "Include city name variations: 'SF', 'San Francisco',
           'San Mateo', 'San Mateo County' for fuzzy match demo"
AI output: Perfect for use case
```

---

## Total Time Breakdown

| Activity | Time Spent | AI Contribution |
|----------|------------|-----------------|
| Architecture Design | 20 min | 70% (I provided context) |
| Database Schema | 10 min | 90% (I reviewed) |
| Mock Data Generation | 5 min | 100% |
| ETL Pipeline Code | 40 min | 80% (I added business logic) |
| Documentation | 30 min | 50% (Heavy editing) |
| Testing & Debugging | 20 min | 0% (Manual) |
| README & AI Prompts Doc | 25 min | 40% (Structure + editing) |
| **Total** | **2.5 hours** | **~60% overall** |

---

## Recommendations for Using AI in Data Engineering

### ‚úÖ Good Use Cases
1. **Boilerplate code** (class structure, error handling patterns)
2. **SQL schema design** (with human review)
3. **Documentation structure** (outline, then edit)
4. **Mock data generation** (saves significant time)
5. **Research** (compare approaches, explain trade-offs)

### ‚ùå Bad Use Cases
1. **Business logic** (requires domain knowledge)
2. **Data validation rules** (needs context)
3. **Security decisions** (too critical)
4. **Production deployment** (environment-specific)
5. **Performance tuning** (needs profiling)

### üéØ Best Practices
1. **Start with context-rich prompts**
2. **Review and understand all AI-generated code**
3. **Iterate: prompt ‚Üí review ‚Üí refine ‚Üí re-prompt**
4. **Edit documentation heavily** (AI is verbose)
5. **Use AI for scaffolding, human judgment for details**

---

## Conclusion

AI (Claude Code) was instrumental in completing this assignment within the 3-hour constraint. However, the **most valuable aspects came from human judgment**:

- Choosing EIA data for business relevance
- Prioritizing fuzzy matching per interview context
- Adjusting feature weights based on domain knowledge
- Documenting shortcuts vs. production practices

**Key Insight**: AI excels at execution; humans excel at direction.

The combination of:
- AI for rapid prototyping (60% of code)
- Human judgment for business context (100% of decisions)
- Clear communication of trade-offs

...resulted in a solution that showcases both technical skills and business understanding.

---

**AI Tool Used**: Claude Code by Anthropic
**Model**: Claude Sonnet 3.5
**Total AI Time**: ~1.5 hours
**Total Human Time**: ~1 hour
**Combined Result**: 2.5-hour take-home submission
